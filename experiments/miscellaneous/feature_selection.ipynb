{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "import os\n",
    "while 'source' not in os.listdir():\n",
    "    os.chdir('..')\n",
    "os.chdir('source')\n",
    "FORCE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "props={\n",
    "    'path'          : 'data',\n",
    "    'seed'          : 42,\n",
    "    'split'         : 0.8,\n",
    "    'test_split'    : 0.5,\n",
    "    'control'       : True,\n",
    "    'huntington'    : False,\n",
    "    'left'          : False,\n",
    "    'right'         : False,\n",
    "    'threshold'     : 0.6,\n",
    "    'binarize'      : True,\n",
    "    'not_connected' : True,\n",
    "    'single'        : None,\n",
    "    'features'      : [],\n",
    "    'features_vox'  : [],\n",
    "    'radiomics'     : [],\n",
    "    'space'         : 'native',\n",
    "    'radiomics_vox' : [\n",
    "        {'im':'t1','fe':['k5_b25','k7_b25','k9_b25','k11_b25','k13_b25','k15_b25','k17_b25','k19_b25','k21_b25']},\n",
    "    ],\n",
    "    'rad_vox_norm'  : 'norm',\n",
    "    'outp'          : 'connectivity',\n",
    "    'balance_data'  : True,\n",
    "    'targets_all'   : False,\n",
    "    'collapse_max'  : False,\n",
    "    'debug'         : False,\n",
    "}\n",
    "architecture={\n",
    "    'activation'    : 'sigmoid',\n",
    "    'layers'        : [2048,1024,512,256,128],\n",
    "    'loss'          : 'CCE',\n",
    "    'learning_rate' : 0.001,\n",
    "    'batch_size'    : 100000,\n",
    "    'patience'      : 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 22:52:04.474439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-04 22:52:04.487379: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-04 22:52:04.491065: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore',category=FutureWarning)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=24576)])\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
   ],
   "source": [
    "from util import getHashId, pickleSave, pickleLoad, getAccuarcy, predictInBatches\n",
    "from ModelClassificationFNN import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "path = props['path']+'/models'\n",
    "\n",
    "def runModel(props):\n",
    "    #get data\n",
    "    gen = DataGenerator(**props)\n",
    "    train, val, test = gen.getData()\n",
    "    #get model id and hash\n",
    "    HASHID, HASH = getHashId(architecture,props)\n",
    "    #compile model\n",
    "    stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=architecture['patience'],\n",
    "    )\n",
    "    save = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=path+'/{}.weights.h5'.format(HASHID),\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "    model = buildModel(train[0].shape[1], train[1].shape[1], activation=architecture['activation'], layers=architecture['layers'])\n",
    "    model.compile(loss=CCE, optimizer=Adam(learning_rate=architecture['learning_rate']), jit_compile=True, metrics=[STD,MAE])\n",
    "    #train model\n",
    "    if FORCE or not os.path.exists(path+'/{}.pkl'.format(HASHID)):\n",
    "        wrapper1 = DataWrapper(train,architecture['batch_size'])\n",
    "        wrapper2 = DataWrapper(val,architecture['batch_size'],False)\n",
    "        history = model.fit(wrapper1,\n",
    "            validation_data=wrapper2,\n",
    "            epochs=10000,\n",
    "            verbose=0,\n",
    "            callbacks = [save,stop],\n",
    "        )\n",
    "        pickleSave(path+'/{}.pkl'.format(HASHID), history.history)\n",
    "    model.load_weights(path+'/{}.weights.h5'.format(HASHID))\n",
    "    #return accuracy\n",
    "    ac = getAccuarcy(val[1],predictInBatches(model,val[0],architecture['batch_size']))\n",
    "    del train\n",
    "    del val\n",
    "    del test\n",
    "    del gen\n",
    "    wrapper1.x = None\n",
    "    wrapper1.y = None\n",
    "    del wrapper1.x\n",
    "    del wrapper1.y\n",
    "    wrapper2.x = None\n",
    "    wrapper2.y = None\n",
    "    del wrapper2.x\n",
    "    del wrapper2.y\n",
    "    del wrapper1\n",
    "    del wrapper2\n",
    "    del model\n",
    "    return ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/MasterThesis/experiments/miscellaneous/.venv/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 4 variables whereas the saved optimizer has 30 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/home/jovyan/MasterThesis/experiments/miscellaneous/.venv/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 26 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733349160.501211  144906 service.cc:146] XLA service 0x7f5fcc005040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1733349160.501234  144906 service.cc:154]   StreamExecutor device (0): NVIDIA H100 80GB HBM3 MIG 7g.80gb, Compute Capability 9.0\n",
      "I0000 00:00:1733349162.684062  144906 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   firstorder_Energy                     57.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/MasterThesis/experiments/miscellaneous/.venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   firstorder_TotalEnergy                58.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m currently_excluded \u001b[38;5;241m=\u001b[39m current_features[i]\n\u001b[1;32m     68\u001b[0m props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures_vox\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m current_features \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m!=\u001b[39m currently_excluded]\n\u001b[0;32m---> 69\u001b[0m ac \u001b[38;5;241m=\u001b[39m \u001b[43mrunModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprops\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ac \u001b[38;5;241m>\u001b[39m current_best:\n\u001b[1;32m     71\u001b[0m     current_best \u001b[38;5;241m=\u001b[39m ac\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mrunModel\u001b[0;34m(props)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrunModel\u001b[39m(props):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#get data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     gen \u001b[38;5;241m=\u001b[39m DataGenerator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprops)\n\u001b[0;32m---> 10\u001b[0m     train, val, test \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#get model id and hash\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     HASHID, HASH \u001b[38;5;241m=\u001b[39m getHashId(architecture,props)\n",
      "File \u001b[0;32m~/MasterThesis/source/DataGeneratorClassificationFNN.py:100\u001b[0m, in \u001b[0;36mDataGenerator.getData\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m         cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpca_obj\u001b[38;5;241m.\u001b[39mexplained_variance_ratio_[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpca_comps]\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpca_comps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetDatapoints(n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames]\n",
      "File \u001b[0;32m~/MasterThesis/source/DataGeneratorClassificationFNN.py:100\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m         cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpca_obj\u001b[38;5;241m.\u001b[39mexplained_variance_ratio_[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpca_comps]\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpca_comps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDatapoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames]\n",
      "File \u001b[0;32m~/MasterThesis/source/DataGeneratorClassificationFNN.py:126\u001b[0m, in \u001b[0;36mDataGenerator.getDatapoints\u001b[0;34m(self, names)\u001b[0m\n\u001b[1;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(x,\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    125\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(y,\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\u001b[38;5;241m.\u001b[39mshuffle(y)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [x, y]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from DataGeneratorClassificationFNN import DataGenerator\n",
    "\n",
    "j0 = 0\n",
    "i0 = 0\n",
    "\n",
    "#load all available features\n",
    "features_oc = np.load(props['path']+'/preprocessed/features_vox.npy')\n",
    "features_ex = []\n",
    "#pretty print\n",
    "features_maxlen = max([len(f) for f in features_oc])\n",
    "def printStatus(ite, fea, ac):\n",
    "    ret = str(ite)\n",
    "    while len(ret) < 4:\n",
    "        ret += ' '\n",
    "    ret += fea\n",
    "    while len(ret) < features_maxlen:\n",
    "        ret += ' '\n",
    "    ret += ' '+str(round(ac*100,1))\n",
    "    print(ret)\n",
    "#keep track of accuracies and excluded features\n",
    "accuracies = []\n",
    "excludeds = []\n",
    "#keep track of best accuracies\n",
    "last_iter_best_idx = 0\n",
    "last_iter_best = 0\n",
    "best_idxs = []\n",
    "#get baseline of all features\n",
    "baseline = runModel(props)\n",
    "print(baseline)\n",
    "accuracies.append(baseline)\n",
    "excludeds.append([])\n",
    "last_iter_best_idx = 0\n",
    "last_iter_best = accuracies[0]\n",
    "best_idxs.append(0)\n",
    "\n",
    "#==== LOAD SAVED ====#\n",
    "if os.path.exists('state.pkl'):\n",
    "    state = pickleLoad('state.pkl')\n",
    "    j0 = state['j']\n",
    "    i0 = state['i']\n",
    "    accuracies = state['accuracies']\n",
    "    excludeds = state['excludeds']\n",
    "    last_iter_best_idx = state['last_iter_best_idx']\n",
    "    last_iter_best = state['last_iter_best']\n",
    "    best_idxs = state['best_idxs']\n",
    "#====================#\n",
    "\n",
    "#top-down exhaustive search\n",
    "max_iter = len(features_oc)\n",
    "for j in range(j0,max_iter):\n",
    "    current_features = [f for f in features_oc if f not in features_ex]\n",
    "    current_best_idx = -1\n",
    "    current_best = 0\n",
    "    for i in range(i0,len(current_features)):\n",
    "        #==== SAVE ====#\n",
    "        state = {\n",
    "            'j':j,\n",
    "            'i':i,\n",
    "            'accuracies':accuracies,\n",
    "            'excludeds':excludeds,\n",
    "            'last_iter_best_idx':last_iter_best_idx,\n",
    "            'last_iter_best':last_iter_best,\n",
    "            'best_idxs':best_idxs,\n",
    "        }\n",
    "        pickleSave('state.pkl',state)\n",
    "        #==============#\n",
    "        currently_excluded = current_features[i]\n",
    "        props['features_vox'] = [f for f in current_features if f != currently_excluded]\n",
    "        ac = runModel(props)\n",
    "        if ac > current_best:\n",
    "            current_best = ac\n",
    "            current_best_idx = len(accuracies)\n",
    "        accuracies.append(ac)\n",
    "        excludeds.append(features_ex+[currently_excluded])\n",
    "        printStatus(i,currently_excluded,ac)\n",
    "    if current_best < last_iter_best:\n",
    "        print('Validation accuracy not increasing, stopping!')\n",
    "        break\n",
    "    print('===================================')\n",
    "    last_iter_best_idx = current_best_idx\n",
    "    last_iter_best = current_best\n",
    "    best_idxs.append(current_best_idx)\n",
    "    features_ex = excludeds[current_best_idx]\n",
    "    print(features_ex)\n",
    "    printStatus(j,features_ex[-1],accuracies[current_best_idx])\n",
    "    print('===================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf14",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "tf14",
   "resource_dir": "/projects/d0a370e9-39db-4874-9788-96e60b5b476d/.local/share/jupyter/kernels/tf14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}