\section{Environmental Aspect}

\subsection{Developement}

The biggest environmental impact of this project was the required computational power to train the models. Over the entire duration of the project, approximately $5,000$ models were trained.\par
As part of it, the most demanding phase was the exhaustive sequential backward feature selection, which in itself required the training of $3,000$ models. The return value of this computation is close to none, as it only marginally increased model performance. The only real benefit is the model being able to reach the same performance using two thirds of the original features. This is a marginal result, because the omitted features belong to different radiomics feature groups, and the biggest overhead for most feature groups is calculating the voxel-based matrix volume, meaning that if not all features are omitted from a feature group, it only decreases the computational requirements marginally.\par
The $3,000$ models for feature selection were trained on a wide variety of GPUs in a network cluster. These GPUs were H100, P40, RTX3060, RTX3090, RTX2080, multiple T4s (Google Collab's), multiple P100s (Kaggle's). Mining the cluster head's log, reveals that around half of these models were trained on the H100 and the other half on the rest. The H100 was consuming around 300 Watts of power while training with a speed of 5s/epoch and taking 30-60 epochs on average to stop. The H100 consumed between $1500 \cdot 0.3$kW $ \cdot 30 \cdot 5$s $ \div 60 \div 60 = 18.75$kWh on the lower and $2 \cdot 18.75 = 37.5$kWh on the upper end. The other half is harder to estimate due to the many different GPUs used in the cluster, but a crude estimate is $1500 \cdot 0.2$kW $ \cdot 30 \cdot 8$s $ \div 60 \div 60 = 20$kWh on the lower and $2 \cdot 20 = 40$kWh on the upper end. Rounding the sum of them up to $80$kWh and adding an extra $30\%$ overhead for running the rest of the components of the servers/PCs, results in an upper boundary of $104$kWh consumed during the feature selection.\par
The remaining $2,000$ models were trained on the P40 and RTX3060, with the P40 training approximately three fourths of them. Doing the same estimations for the P40 yields $1500 \cdot 0.2$kW $ \cdot 30 \cdot 7$s $ \div 60 \div 60 = 17.5$kWh on the lower and $2 \cdot 17.5 = 35$kWh on the upper end. For the RTX3060, it yields $500 \cdot 0.15$kW $ \cdot 30 \cdot 7$s $ \div 60 \div 60 = 4.375$kWh on the lower and $2 \cdot 4.375 = 8.75$kWh on the upper end. Rounding their sum up to $45$kWh and adding an extra $30\%$ overhead for running the rest of the components of the server, results in an upper boundary of $58.5$kWh consumed during the the rest of the trainings.\par
Feature extraction run on a CPU, with the entire server consuming 200 Watts during the process. The entire duration for the feature extraction run was around 2 weeks. Its total power consumption was $0.2$kW $ \cdot 14$d $ \cdot 24$h $ = 67.2$kWh.\par
In summary, this project had the environmental impact summarized in \reflink{tab:sus1}{Table}, for an average of $0.25$€/kWh for the price of the electricity \cite{kwhprice} and $0.2$kgCO$_{2}$e/kWh for the emission \cite{carbon}:
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Part} & \textbf{Consumption} & \textbf{Price} & \textbf{Emission} \\ \hline
feature selection & $104$kWh & $26$€ & $20.8$kgCO$_{2}$e \\ \hline
rest of the experiments & $58.5$kWh & $14.6$€ & $11.7$kgCO$_{2}$e \\ \hline
feature extraction & $67.2$kWh & $16.8$€ & $13.4$kgCO$_{2}$e \\ \hline
\textbf{Total} ($+10\%$ overhead; rounded up) & $260$kWh & $65$€ & $52$kgCO$_{2}$e \\ \hline
\end{tabular}
\caption{Sustainability: Power Consumptions and CO$_{2}$ Emissions}
\label{tab:sus1}
\end{table}
For reference, this number is a bit less than the average Spanish household's monthly power consumption of $324$kWh. \cite{kwhavg}

\subsection{Production}
\label{sec:susprod}

This project, whether implemented in a production, clinical or research setting, has the potential to significantly enhance sustainability in its respective fields. Acquiring \ac{DTI} data typically requires up to 40 minutes, depending on the specific parameters, whereas obtaining a T1 image takes only about 8 minutes, making data acquisition a considerable time-saving step. Additionally, \ac{DTI} data demands extensive and labor-intensive preprocessing. \textbf{Accelerating clinical or research workflows by a factor of 5} not only saves time but also increases the capacity for data collection or patient processing by the same factor.\par
Synthesizing a single record of relative connectivity or diffusion \ac{FA}/\ac{MD} takes only a few seconds on a GPU, while the radiomic feature extraction takes around 20 minutes. This translates into an approximate energy demand of $200$W $ \cdot (5$s $ \div 60 + 20$m$) \div 60 = 67$Wh for synthetizing a record. It can be argued that the time saved with the data acquisition is lost with the time it takes to extract the radiomic features, but neither the doctor (or MRI operator) and patient need to be present during the computation of the feature extraction. Assuming that an \ac{MRI} machine has a consumption of $25$-$70$kW \cite{kwhmri} and calculating with an average of $50$kW, adds up to a power requirement of $50$kW $ \cdot 40$m $ \div 60 = 33.3$kWh per \ac{DTI} record. Adding up the total energy requirements of T1 data acquisition and record synthesis is $50$kW $ \cdot 8$m $ \div 60 + 67$Wh $ \div 1000 = 6.7$kWh per record. This means that \textbf{this method could be 5 times more sustainable than the traditional approach}, while returning the energy consumption of the development phase in the data acquisition of $260 \div (33.3 - 6.7) = 10$ records, which is naturally a minuscule number when compared to how many \ac{MRI} recordings are routinely acquired in the world.

\section{Economic Aspect}

\subsection{Cost}

The electricity cost during the development phase amounted to $65$€. Approximately half of this cost was covered by colleagues and their networks, who contributed computational resources during the feature selection process. Additional support came from free services provided by Google and Kaggle.\par
Realistically, billing this project would consist of three components: the cost of labor, the price of the server used for development, and the electricity consumed. With the electricity cost already detailed, the server cost and labor are interconnected. The project could have been executed on a less powerful machine, though this would have required more time and more careful focus on software design. Throughout the development, the primary limiting factors were RAM and disk storage. Significant effort was invested in developing efficient data structures and optimizing the memory footprint to address these constraints.\par
In conclusion, the minimum hardware which this project could be feasibly executed on is 32GBs of RAM, Intel Core i5-12600K (or similar), Nvidia RTX3060 (or similar), and $256$GBs of storage. The current cost (new parts ordered from Amazon) of such server (with 'cheap' consumer grade hardware) would be around $300$€ for the GPU, $150$€ for the CPU, $50$€ for the RAM, $25$€ for the SSD, and $150$€ for the motherboard, adding up to $675$€. The main limiting factors in this configuration are the RAM, which was utilized up to $128$GBs during development; and disk storage, which was utilized up to $300$GBs. The extra RAM and storage would cost an additional $200$€ and $25$€ totalling at a new sum of $900$€.\par
The development of this project required approximately $500$ hours of work. Making calculations using the average hourly rate of $19$€ corresponding to a software engineer in Spain \cite{salary}, leads to a total labor cost of $19$€ $ \cdot 500$h $ = 9500$€. Therefore, the total billing for the project would be the one summarized in \reflink{tab:sus2}{Table}:
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Part} & \textbf{Price} \\ \hline
Cumulative Salary & $9500$€ \\ \hline
Server Components & $900$€ \\ \hline
Electricity & $65$€ \\ \hline
\textbf{Total} & $10465$€ \\ \hline
\end{tabular}
\caption{Project Cost Breakdown}
\label{tab:sus2}
\end{table}

\subsection{Return}

This project is by all means a proof of concept, aiming to assess the viability of the proposed approach. More information on the exact conclusions can be found in \reflink{sec:conclusions}{Chapter}, but the potential return value of this project is huge.\par
As stated in the previous \reflink{sec:susprod}{subsection}, this project has the potential to accelerate the clinical and/or research workflows by a factor of 5. The economic implications are applicable for both clinical and research workflows. In the clinical workflow, \textbf{this could increase the number of patients processed by a factor of 5}, with minimal extra cost (especially if compared to the alternative of buying 4 additional \ac{MRI} machines and hiring operators for them). In a research workflow, it would probably have less of an economic impact, but more of a logistical one, as it could simplify the data acquisition, thus \textbf{allowing researchers to have up to 5 times more data acquired} with minimal extra resources invested. But even more importantly, it opens the door of processing past anatomical \ac{MRI} records, as they are much more common than \ac{DTI} records, virtually \textbf{increasing the available data for researchers by several orders of magnitude} (depending on the exact application).

\section{Social Aspect}

\subsection{Inclusivity}
\label{sec:inclusive}

Out of the potential inclusivity aspects, including age, gender, sex, and cultural diversity, it is sex the one that requires most attention, as males and females have slightly different brain structures. \cite{sexbrain} For the control records used, there are 17 male and 15 female ones, which is a relatively even split; for the patient records, there are 25 male and 13 female records, which is relatively unbalanced. This could, in theory, negatively impact the under-represented group if there were truly any differences between the two sexes that mattered from the model’s perspective. However, in this project, and due to the very limited number of records available, it was not an option to discard one third of the patients in order to have the a completely balanced dataset. Moreover, it would need further experimentation so as to determine if this aspect truly impacts the results.\par
This potential difference was overlooked during most of the project’s lifetime, but it should have been included as another constant ratio to be kept during the train/validation/test splitting, which was covered in \reflink{sec:travaltes}{subsection}. Due to limited time for thesis completion and resources, the models were not re-trained with this new rule in mind, and this is something that has been included in the \reflink{sec:improve}{Future Improvements} subsection.\par
The rest of the considerations are less pressing and hard to take into account, like the brain structure of different ethnicities, given that this dataset does not contain any information regarding this aspect. The only other aspect potentially under consideration should be age, but this is partially accounted for as part of the constant symptomatic/asymptomatic ratio, because being symptomatic is closely related to the \ac{CAP} score (covered in \reflink{sec:clinical}{subsection}), which is directly related to age. Thus symptomatic/asymptomatic is indirectly related to age.

\section{Risks}

Environmental, Economic and Social risks are not really applicable to this project, as it is a foundational proof of concept research project.








